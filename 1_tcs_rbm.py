# -*- coding: utf-8 -*-
"""1_TCS_RBM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OdlIY1oz600KNbJqmh4zr-S0LPdVD097

###**RULE-BASED MODEL**
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/MyDrive/Resources

"""### Importing Libraries"""

import pandas as pd
import numpy as np
 
import nltk                           # natural language toolkit
from nltk.corpus import stopwords     #A stop list is a list of words that are excluded from some language processing task, usually because they are viewed as non--informative or potentially misleading. Usually they are non--content words like conjunctions, determiners, prepositions, etc.


from tqdm._tqdm_notebook import tqdm_notebook
tqdm_notebook.pandas()                          #for progress-bar of loops
from gensim.models import FastText              # taking account of morphological structure(in depth suffix,prefix..) of a word carries important information about the meaning of the word.


from contractions import contraction_map          
import string,re

from nltk.stem import WordNetLemmatizer            # for linking words with similar meanings together eg. good and better, rocks and rock it takes pos  parameter, if not supplied, the default is “noun.”

import seaborn as sns                             # better plotting in py
import matplotlib.pyplot as plt                    # basic plotting in py

nltk.download('stopwords')
nltk.download('wordnet')

data = pd.read_csv('/content/drive/MyDrive/Resources/text_emotion.csv')         # importing dataset

"""### Data Preprocessing"""

data.head()

df = data.copy()     # Modifications to the data or indices of the copy will not be reflected in the original object

data.sentiment.value_counts()            #returns a series containing counts of unique values

plt.figure(figsize=(12,5))
sns.countplot(x='sentiment', data=df)
plt.show()

df.dtypes

# Dropping rows with other emotion labels, i.e rounding number of emotions to only 5 basic ones 

df = df.drop(df[df.sentiment == 'boredom'].index)
df = df.drop(df[df.sentiment == 'enthusiasm'].index)
df = df.drop(df[df.sentiment == 'empty'].index)
df = df.drop(df[df.sentiment == 'fun'].index)
df = df.drop(df[df.sentiment == 'relief'].index)
df = df.drop(df[df.sentiment == 'surprise'].index)
df = df.drop(df[df.sentiment == 'love'].index)
df = df.drop(df[df.sentiment == 'hate'].index)

df.sentiment.value_counts()                     # count of new values

df

df = df.drop('author',axis=1)          #DImensionality Reduction to remove 'author' feature
df

df.reset_index(drop=True,inplace=True)            # To reorder new tuples and attributes

df

df.shape                   # 27581 rows * 3 columns

df.drop('tweet_id',axis=1,inplace=True)

df.columns = ['sentiment','text']

df.head()

"""### Data Cleaning"""

#Using Sentiment lexicons to be excluded from stopwords

df_pos = pd.read_csv('./lexicons/positive.csv')
df_neg = pd.read_csv('./lexicons/negative.csv')

def expand_text(text):
    text = text.lower()
    text = text.replace("`","'")
    
    #Expand Contractions
    contraction_dict = contraction_map
    contraction_keys = list(contraction_dict.keys())
    
    for word in text.split():
        if word in contraction_keys:
            text = text.replace(word, contraction_dict[word])
        else:
            continue
    
    return text

def clean_text(text):
    text = text.translate(string.punctuation)
    text = text.lower().split()
    
    df_pos_words = list(df_pos.words)
    df_neg_words = list(df_neg.words)
    
    positive = []
    for i in range(0,len(df_pos_words)):
        positive.append(df_pos_words[i].lower().replace(" ",""))
        
    negative = []
    for i in range(0,len(df_neg_words)):
        negative.append(df_neg_words[i].lower().replace(" ",""))
        
    pos_set = set(positive)
    neg_set = set(negative)
    
    keywords = set(["above","and","below","not"])
    
    keywords.update(pos_set)
    keywords.update(neg_set)
    
    stopwords_set = set(stopwords.words('english'))
    stops = stopwords_set - keywords
    
    
    text = [w for w in text if not w in stops]
    text = " ".join(text)
    
    text = re.sub(r"[^A-Za-z0-9^,!./\'+-=]"," ",text)
    text = re.sub(r"what's","what is",text)
    text = re.sub(r"\'s"," ",text)
    text = re.sub(r"\'ve"," have ",text)
    text = re.sub(r"n't"," not ",text)
    text = re.sub(r"i'm"," i am ",text)
    text = re.sub(r"\'re"," are ",text)
    text = re.sub(r"\'d", " would ",text)
    text = re.sub(r"\'ll", " will ",text)
    text = re.sub(r","," ",text)
    text = re.sub(r"\."," ",text)
    text = re.sub(r"!"," ! ",text)
    text = re.sub(r"\/"," ",text)
    text = re.sub(r"\^"," ^ ",text)
    text = re.sub(r"\+"," + ",text)
    text = re.sub(r"\-"," - ",text)
    text = re.sub(r"\="," = ",text)
    text = re.sub(r"'"," ",text)
    text = re.sub(r"(\d+)(k)",r"\g<1>000",text)
    text = re.sub(r":", " : ",text)
    text = re.sub(r" e g "," eg ",text)
    text = re.sub(r"b g "," bg ",text)
    text = re.sub(r" u s "," american ",text)
    text = re.sub(r"\0s","0",text)
    text = re.sub(r"e - mail","email",text)
    text = re.sub(r"\s{2,}"," ",text)
    
    text = text.split()
    lemmatizer = WordNetLemmatizer()
    lemmatized_words = [lemmatizer.lemmatize(word) for word in text]
    text = " ".join(lemmatized_words)
    
    return text

df['text'] = df['text'].progress_apply(lambda x : expand_text(x))

df['text'] = df['text'].progress_apply(lambda x: clean_text(x))

df.sentiment.value_counts()

df

"""###Feature encoding"""

from sklearn.preprocessing import LabelEncoder
lbl_enc = LabelEncoder()

y = lbl_enc.fit_transform(df.sentiment.values) #Label encoding

#For building a dataframe for mapping emotions to label number
y_series = pd.Series(y)
emo_cols_series = pd.Series(lbl_enc.inverse_transform(y))  #inverse transform to find  mapped emotions

emo_df = pd.DataFrame()
emo_df = pd.concat([emo_cols_series,y_series],axis=1)
emo_df.columns=['emotion','Label_mapped']

emo_df = emo_df.drop_duplicates()
emo_df.sort_values(by=['Label_mapped'],ascending= True)

"""## Model building

###Importing Libraries
"""

#Train test split 
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer            # term frequency-inverse document frequency
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

X_train,X_test,y_train,y_test = train_test_split(df.text.values,df.sentiment.values,random_state=42, test_size=0.1, shuffle=True)  # splitting

"""### Using Tf-idf - Term Frequency - Inverse Document Frequency"""

#Extracting Tf-idf features                            # X-Train is text values .. Y_Train is sentiment values 

tfidf = TfidfVectorizer(min_df=3,  max_features=None, 
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1)
tfidf.fit(list(X_train) + list(X_test))

X_train_tfidf = tfidf.transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

"""### Using Countvectorizer"""

#Extracting Counvectorizer features

count_vec = CountVectorizer(analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3))
count_vec.fit(list(X_train) + list(X_test))

X_train_count_vec = count_vec.transform(X_train)
X_test_count_vec = count_vec.transform(X_test)

"""## Final Result:

###Using TF-IDF for models
"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=500)
rf.fit(X_train_tfidf,y_train)
y_pred2 = rf.predict(X_test_tfidf)
print('Random forest tfidf accuracy %s' % accuracy_score(y_pred2, y_test))

from sklearn.linear_model import SGDClassifier
lsvm = SGDClassifier(alpha=0.001, random_state=5, max_iter=15, tol=None)
lsvm.fit(X_train_tfidf, y_train)
y_pred_sgd = lsvm.predict(X_test_tfidf)
print('SGD using tfidf accuracy %s' % accuracy_score(y_pred_sgd, y_test))

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=1.0)
lr.fit(X_train_tfidf,y_train)
y_pred = lr.predict(X_test_tfidf)
print('Accuracy with Logistic Regression and Tf-idf is {}'.format(accuracy_score(y_pred,y_test)))

"""###Using Countvec for models"""

from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB
nb = MultinomialNB()
nb.fit(X_train_count_vec, y_train)
y_pred = nb.predict(X_test_count_vec)
print('naive bayes count_vec accuracy %s' % accuracy_score(y_pred, y_test))

from sklearn.linear_model import SGDClassifier
lsvm = SGDClassifier()
lsvm.fit(X_train_count_vec, y_train)
y_pred = lsvm.predict(X_test_count_vec)
print('svm using countvec accuracy %s' % accuracy_score(y_pred, y_test))

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(C=1.0)
lr.fit(X_train_count_vec,y_train)
y_pred = lr.predict(X_test_count_vec)
print('Accuracy with Logistic Regression and CountVec is {}'.format(accuracy_score(y_pred,y_test)))

"""##Generate classification report and confusion matrix for SGD + TF-IDF model"""

from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred_sgd))